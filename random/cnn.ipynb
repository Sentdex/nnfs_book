{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "from activation_func.relu import Activation_ReLU\n",
    "from layers.layer_dense import Layer_Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random thinking about matrix multiplications lol \n",
    "x=np.random.randn(2,784)\n",
    "layer1=Layer_Dense(784,200)\n",
    "relu=Activation_ReLU()\n",
    "layer1.forward(x,training=True)\n",
    "relu.forward(layer1.output,training=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#padding is adding zeroes to the image matrix white it is being processed\n",
    "So output matrix will have more knowledge about the middle part of the image than edges and we might lose some important features from edges.\n",
    "so two downsides are shrinking and edges are not used a lot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zero padding for height and width dimensions     \n",
    "#shape out == [n+2*pad] for given dims which are padded\n",
    "def zero_padding(inputs,padding):\n",
    "    out=np.pad(inputs,((0,0),(padding,padding),(padding,padding),(0,0)),mode='constant',constant_values=(0,0))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape after padding (4, 6, 6, 3)\n",
      "shape match  True\n"
     ]
    }
   ],
   "source": [
    "#out shape should be  \n",
    "images=np.random.randn(4,6,6,3)\n",
    "padding=0\n",
    "out=zero_padding(images,padding)\n",
    "#compare the shapes [n+2p]\n",
    "print('shape after padding',out.shape)\n",
    "print('shape match ',out.shape[1]==images.shape[1]+(2*padding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.91085472 5.02681643 6.136079   5.99971182]\n",
      " [6.148262   6.46462906 7.33183787 6.53712559]\n",
      " [5.9483094  7.94525324 7.37590482 6.79068827]\n",
      " [7.12852876 6.57504757 6.82846091 7.36506999]]\n",
      "relu output shape (4, 4)\n"
     ]
    }
   ],
   "source": [
    "#3D convolution example with one filter and relu activation function\n",
    "image=np.random.rand(6,6,3)\n",
    "W1=np.random.rand(3,3,3)\n",
    "relu =Activation_ReLU()\n",
    "\n",
    "(n_h_prev,n_w_prev,n_c_prev)=image.shape\n",
    "(f,f,n_c_prev)=W1.shape\n",
    "\n",
    "#output shape [N-F+1]\n",
    "n_h=int(n_h_prev-f+1)\n",
    "n_w=int(n_w_prev-f+1)\n",
    "\n",
    "#init output with zeroes  \n",
    "output=np.zeros((n_h,n_w))\n",
    "\n",
    "for h in range(n_h):\n",
    "    for w in range(n_w):\n",
    "        #define vert-horiz start end for convolution\n",
    "        vert_start=h\n",
    "        vert_end=vert_start+f\n",
    "        horiz_start=w\n",
    "        horiz_end=w+f\n",
    "        #slice of the image to perform convolution \n",
    "        a_prev=image[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "        #convolution and store value \n",
    "        output[h,w]=np.sum(np.multiply(W1,a_prev))\n",
    "\n",
    "relu.forward(output,training=True)\n",
    "print(output)\n",
    "print('relu output shape',relu.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape (2, 2)\n",
      "output values [[1.59919375 1.62622864]\n",
      " [1.17751356 2.18563288]]\n"
     ]
    }
   ],
   "source": [
    "#2D convolution example with one filter \n",
    "image=np.random.rand(4,4)\n",
    "filter=np.random.rand(3,3)\n",
    "\n",
    "#shapes\n",
    "(n_h_prev,n_w_prev)=image.shape\n",
    "(f,f)=filter.shape\n",
    "\n",
    "#output shape\n",
    "n_h=int(n_h_prev-f+1)\n",
    "n_w=int(n_w_prev-f+1)\n",
    "\n",
    "#output\n",
    "output=np.zeros((n_h,n_w))\n",
    "\n",
    "for h in range(n_h):\n",
    "    for w in range(n_w):\n",
    "        #vertical/horizontal start and end\n",
    "        vert_start=h\n",
    "        vert_end=vert_start+f\n",
    "        horiz_start=w\n",
    "        horiz_end=horiz_start+f\n",
    "\n",
    "        image_slice=image[vert_start:vert_end,horiz_start:horiz_end]\n",
    "        output[h,w]=np.sum(np.multiply(image_slice,filter))\n",
    "\n",
    "print('output shape',output.shape)\n",
    "print('output values',output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one layer of convolution\n",
    "#3d convolution with batch of images and multiple filters/bias  and relu act/func\n",
    "\n",
    "def conv_forward(inputs,w1,b1,padding,stride):\n",
    "        \n",
    "    #get shapes\n",
    "    (m,n_h_prev,n_w_prev,n_c_prev)=inputs.shape\n",
    "    (f,f,n_c_f_prev,n_c)=w1.shape\n",
    "\n",
    "    #define output shape\n",
    "    n_h=int((n_h_prev-f+2*padding)/stride)+1\n",
    "    n_w=int((n_w_prev-f+2*padding)/stride)+1\n",
    "\n",
    "    #init output tensor \n",
    "    output=np.zeros((m,n_h,n_w,n_c)) \n",
    "\n",
    "    #padding \n",
    "    inputs_pad=zero_padding(inputs,padding)\n",
    "\n",
    "    for i in range(m):\n",
    "        inputi=inputs_pad[i]\n",
    "        for h in range(n_h):\n",
    "            for w in range(n_w):\n",
    "                for c in range(n_c):\n",
    "                    #define vertical/horizontal start-end \n",
    "                    vert_start=h*stride\n",
    "                    vert_end=vert_start+f\n",
    "                    horiz_start=w*stride\n",
    "                    horiz_end=horiz_start+f\n",
    "                    \n",
    "                    #get image slice\n",
    "                    input_slice=inputi[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "                    #convolution and store to a new tensor values\n",
    "                    output[i,h,w,c]=np.sum(np.multiply(input_slice,w1[...,c]))+b1[...,c]\n",
    "\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from activation_func.softmax import Activation_Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of conv1 output  (3, 37, 37, 10)\n",
      "shape of conv2 output  (3, 17, 17, 20)\n",
      "shape of conv3 output  (3, 7, 7, 40)\n",
      "shape after flattening  (3, 1960)\n",
      "shape match  True\n"
     ]
    }
   ],
   "source": [
    "#simple convolution network example:conv1,conv2,conv3,softmax\n",
    "inputs=np.random.rand(3,39,39,3)\n",
    "#convolution layer1\n",
    "relu1=Activation_ReLU()\n",
    "w1=np.random.rand(3,3,3,10)\n",
    "b1=np.random.rand(1,1,1,10)\n",
    "stride1=1\n",
    "padding1=0\n",
    "#convolution layer2\n",
    "relu2=Activation_ReLU()\n",
    "w2=np.random.rand(5,5,10,20)\n",
    "b2=np.random.rand(1,1,1,20)\n",
    "stride2=2\n",
    "padding2=0\n",
    "#convolution layer3\n",
    "relu3=Activation_ReLU()\n",
    "softmax=Activation_Softmax()\n",
    "w3=np.random.rand(5,5,20,40)\n",
    "b3=np.random.rand(1,1,1,40)\n",
    "stride3=2\n",
    "padding3=0\n",
    "\n",
    "#convolution layer1 forward  \n",
    "conv_output1=conv_forward(inputs,w1,b1,padding1,stride1)\n",
    "relu1.forward(conv_output1,training=True)\n",
    "print('shape of conv1 output ',relu1.output.shape)\n",
    "\n",
    "#convolution layer2 forward \n",
    "conv_output2=conv_forward(relu.output,w2,b2,padding2,stride2)\n",
    "relu2.forward(conv_output2,training=True)\n",
    "print('shape of conv2 output ',relu2.output.shape)\n",
    "\n",
    "#convolution layer3 forward \n",
    "conv_output3=conv_forward(relu2.output,w3,b3,padding3,stride3)\n",
    "relu3.forward(conv_output3,training=True)\n",
    "print('shape of conv3 output ',relu3.output.shape) \n",
    "\n",
    "#flatten the relu3 output to have shape [3,7*7*40] \n",
    "x=relu3.output.reshape(relu3.output.shape[0],-1)\n",
    "print('shape after flattening ',x.shape)\n",
    "#should be [3,1960]\n",
    "print('shape match ',x.shape[1]==relu3.output.shape[1]*relu3.output.shape[2]*relu3.output.shape[3])\n",
    "#feed x into softmax to get probability distributions \n",
    "softmax.forward(x,training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape before max pooling  (15, 15)\n",
      "inputs shape after max pooling  (7, 7)\n"
     ]
    }
   ],
   "source": [
    "#max pooling layer example on 3d image\n",
    "inputs=np.random.rand(15,15)\n",
    "f=2\n",
    "stride=2\n",
    "#get inputs shape\n",
    "(n_h_prev,n_w_prev)=inputs.shape\n",
    "\n",
    "#define output shapes\n",
    "n_h=int((n_h_prev-f)/stride)+1\n",
    "n_w=int((n_w_prev-f)/stride)+1\n",
    "\n",
    "#output\n",
    "output=np.zeros((n_h,n_w))\n",
    "\n",
    "for h in range(n_h):\n",
    "    for w in range(n_w):\n",
    "        #outputs \n",
    "        vert_start=h*stride\n",
    "        vert_end=vert_start+f\n",
    "        horiz_start=w*stride\n",
    "        horiz_end=horiz_start+f\n",
    "\n",
    "        inputs_slice=inputs[vert_start:vert_end,horiz_start:horiz_end]\n",
    "        output[h,w]=np.max(inputs_slice)\n",
    "\n",
    "print('inputs shape before max pooling ',inputs.shape)\n",
    "print('inputs shape after max pooling ',output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max pooling over batch of images\n",
    "def max_pool(inputs,f,stride):\n",
    "    #get shapes\n",
    "    (m,n_h_prev,n_w_prev,n_c_prev)=inputs.shape\n",
    "    \n",
    "    #define output shape\n",
    "    n_h=int((n_h_prev-f)/stride)+1\n",
    "    n_w=int((n_w_prev-f)/stride)+1\n",
    "    \n",
    "    #output\n",
    "    output=np.zeros((m,n_h,n_w,n_c_prev))\n",
    "\n",
    "    for i in range(m):\n",
    "        inputi=inputs[i]\n",
    "        for h in range(n_h):\n",
    "            for w in range(n_w):\n",
    "                for c in range(n_c_prev):\n",
    "                    #define vert/horiz start-end  \n",
    "                    vert_start=h*stride\n",
    "                    vert_end=vert_start+f\n",
    "                    horiz_start=w*stride\n",
    "                    horiz_end=horiz_start+f\n",
    "                    #get image slice \n",
    "                    inputi_slice=inputi[vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                    #max pooling on inputs[i] slice\n",
    "                    output[i,h,w,c]=np.max(inputi_slice)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of inputs before max-pool  (3, 15, 15, 3)\n",
      "shape of inputs after max-pool  (3, 7, 7, 3)\n"
     ]
    }
   ],
   "source": [
    "inputs=np.random.rand(3,15,15,3)\n",
    "#hyperparameters shrinking by a factor of 2 \n",
    "f=2\n",
    "stride=2\n",
    "output=max_pool(inputs,f,stride)\n",
    "\n",
    "print('shape of inputs before max-pool ',inputs.shape)\n",
    "print('shape of inputs after max-pool ',output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 10, 10, 16)\n",
      "shape of inputs  (5, 32, 32, 3)\n",
      "shape of 1st layer max-out  (5, 14, 14, 6)\n",
      "shape of 2st layer max-out  (5, 5, 5, 16)\n",
      "shape of inputs flattened  (5, 400)\n",
      "shape of 3st fc relu   (5, 120)\n",
      "shape of 4th fc relu out   (5, 84)\n",
      "shape of 5th fc relu out   (5, 10)\n",
      "final softmax shape  (5, 10)\n"
     ]
    }
   ],
   "source": [
    "#CNN example forward pass ||something similar to LeNet-5  \n",
    "inputs=np.random.rand(5,32,32,3)\n",
    "#layer1 (conv,relu,max-pool)\n",
    "relu1=Activation_ReLU()\n",
    "w1=np.random.rand(5,5,3,6)\n",
    "b1=np.random.rand(1,1,1,6)\n",
    "stride1=1\n",
    "padding1=0\n",
    "\n",
    "#forward pass layer1(convolution + relu)\n",
    "conv1_out=conv_forward(inputs,w1,b1,padding1,stride1)\n",
    "relu1.forward(conv1_out,training=True)\n",
    "#layer1 max pooling \n",
    "max_pool1=max_pool(relu1.output,f=2,stride=2)\n",
    "\n",
    "#layer2\n",
    "relu2=Activation_ReLU()\n",
    "w2=np.random.rand(5,5,6,16)\n",
    "b2=np.random.rand(1,1,1,16)\n",
    "stride2=1\n",
    "padding2=0\n",
    "#layer2 forward pass(conv,relu,max-pool)\n",
    "conv2_out=conv_forward(max_pool1,w2,b2,padding2,stride2)\n",
    "relu2.forward(conv2_out,training=True)\n",
    "print(relu2.output.shape)\n",
    "max_pool2=max_pool(relu2.output,f=2,stride=2)\n",
    "\n",
    "#flatten/reshape the max-pool2\n",
    "x=max_pool2.reshape(max_pool2.shape[0],-1)\n",
    "\n",
    "#fully-connected layer3\n",
    "relu3=Activation_ReLU()\n",
    "layer3=Layer_Dense(400,120)\n",
    "#forward pass layer3(linear-layer,relu)\n",
    "layer3.forward(x,training=True)\n",
    "relu3.forward(layer3.output,training=True)\n",
    "\n",
    "#fully-connected layer4\n",
    "layer4=Layer_Dense(120,84)\n",
    "relu4=Activation_ReLU()\n",
    "layer4.forward(relu3.output,training=True)\n",
    "relu4.forward(layer4.output,training=True)\n",
    "\n",
    "#fully-connected layer5\n",
    "layer5=Layer_Dense(84,10)\n",
    "relu5=Activation_ReLU()\n",
    "layer5.forward(relu4.output,training=True)\n",
    "relu5.forward(layer5.output,training=True)\n",
    "\n",
    "\n",
    "#softmax probability distribution \n",
    "softmax=Activation_Softmax()\n",
    "softmax.forward(relu5.output,training=True)\n",
    "\n",
    "print('shape of inputs ',inputs.shape)\n",
    "print('shape of 1st layer max-out ',max_pool1.shape) \n",
    "print('shape of 2st layer max-out ',max_pool2.shape)\n",
    "print('shape of inputs flattened ',x.shape)\n",
    "print('shape of 3st fc relu  ',relu3.output.shape)\n",
    "print('shape of 4th fc relu out  ',relu4.output.shape)\n",
    "print('shape of 5th fc relu out  ',relu5.output.shape)\n",
    "print('final softmax shape ',softmax.output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
